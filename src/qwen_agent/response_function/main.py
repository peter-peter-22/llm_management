import json

from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool


@register_tool('response')
class ResponseFunction(BaseTool):
    description = 'Tell me about Canada.'
    parameters = [{
        "type": "object",
        "properties": {
            "name": {"type": "string"},
            "capital": {"type": "string"},
            "languages": {
                "type": "array",
                "items": {"type": "string"}
            }
        },
        "required": ["name", "capital", "languages"]
    }]

    def call(self, params: str, **kwargs):
        # `params` are the arguments generated by the LLM agent.
        data = json.loads(params)
        print(data)


llm_cfg = {
    'model': 'qwen2.5:3b',
    'model_server': 'http://localhost:11434/v1',
    'api_key': 'EMPTY',

    'generate_cfg': {
        'top_p': 0.8,
        'temperature': 0.0,
        'max_tokens': 2048,
    }
}

system_instruction = '''Respond with the response tool.'''
bot = Assistant(
    llm=llm_cfg,
    system_message=system_instruction,
    function_list=[ResponseFunction()]
)

messages = [{'role': 'user', 'content': "Tell me about Canada."}]
responses = ''
for responses in bot.run(messages=messages):
    pass
print(responses)
# The tool chain doesn't end with the tool call, the mode will have to respons with a text message after calling it. The Assistant has no official support for structured response.
